## Introduction


这个模块介绍语言模型的基本概念——也就是语音识别系统计算先验概率$P(W)$的部分。回忆一下语音识别系统的基本公式，我们组合声学模型概率$P(O \vert W)$和语言模型概率$P(W)$来寻找最可能的假设：

$$
\hat{W} = argmax_{W} P( O | W) P(W)
$$

因此，语言模型(或者简称LM)包含了词序列的先验概率，即使我们没有听到实际的语音也可以计算它。传统的文法(比如正则文法、上下文无关文法)通过硬编码的规则来定义哪些句子是合乎语法的哪些是不合乎的(就像C语言编译器检查C程序是否合乎语法一样)，现在更主流的方法是使用概率的模型，它会给更可能的句子更高的概率而不太可能的句子较低的概率，但是即使很罕见的句子也可能有极小的概率(因为没有人知道说话人实际会说什么东西，他甚至会说完全不合乎语法的句子)。而且统计模型的这些概率不是有语言学专家来指定这些概率。和声学模型一样，我们通过大量的数据来估计模型的参数。因此我们通过实际的训练数据的统计来觉得哪些词序列在某个特定的语言、场景和应用下更加可能。

注意：在语言模型里我们说的句子是一段utterance的词序列，它不一定是完全语法正确的句子，甚至不一定完整。

## N-gram语言模型

### Vocabulary

我们需要计算每个句子的概率，一个句子就是一个词的序列：

$$
W = w_1 w_2 \ldots w_n
$$
 
 

这里n是词的个数，理论上是没有上限的。为了简化问题，我们首先假定词的个数是有限的，也就是LM的词典是一个有限集合。注意：LM的词典也是语音识别系统的词典——我们不可能识别一个语言模型里没有的词。

词典之外的词叫做未登录(OOV)词。如果出现了一个OOV，那么至少会出现一个语音识别错误，因此我们需要选择合适的词典来尽量减少OOV。一个常见的策略是从数据中估计每个词出现的先验概率，然后选择概率高的词。也就是说，我们选择训练数据中出现频率最高的那些词。词典越大，OOV就越小，识别的准确率(可能，但不一定，因为加入很罕见的词反而会降低准确率，因为这些罕见可能会和正常的词混淆)会更高；但是太大的词典会导致模型过大，从而训练和解码速度变慢。

### 马尔科夫分解和N-Gram

即使词典是有限的，我们仍然会面对句子长度(理论上)无限的问题，因此我们不可能穷尽所有的可能(类似类似的概率分布函数)。而且即使我们理论上可以这么做，其效果也是很差的，因为很多句子在训练数据中不可能一字不差的出现。

我们可以使用和声学建模类似的trick来解决这些问题：使用链式法则把句子的概率分解成很多条件概率的乘积，然后使用马尔科夫假设把条件(历史)限制在有限的长度上。

$$
P(W) = P(w_1) \times P(w_2 |w_1) \times P(w_3 | w_1 w_2)
\times \ldots \times P(w_n | w_1 \ldots w_{n-1})
$$

限制我们使用一阶的马尔科夫假设，假设一个词的概率只依赖于之前的一个词(history)：

$$
P(W) = P(w_1) \times P(w_2 |w_1) \times P(w_3 | w_2) \times
\ldots \times P(w_n | w_{n-1})
$$


注意每个词值依赖于前面的那个词，也就是我们使用了一阶的马尔科夫模型。但是在语言模型里，我们不太喜欢使用这个术语，在这里我们喜欢叫它bigram模型，英文在统计的时候只考虑相邻两个词。类似的，二阶马尔科夫模型对应trigram，它是根据前面两个词来预测当前词。

上面这种方法的推广就是N-gram模型，也就是每个词依赖于前面的N-1个词。从trigram的效果要比bigram好很多，但是再增加N效果就不那么明显了，而且模型的大小(参数)增加的很多。因此在实践中我们很少使用4-gram或者5-gram。在本模块的实验里我们使用trigram，而在其它部分为了简化概念我们大部分时候使用bigram为例子。但是这些理论对于N-gram都是适用的。


### 句子开始和结束

为了让我们的N-gram语言模型能够对所有有限长的句子都赋予一个概率，我们面临一个小问题：怎么建模句子的结束？一种办法是我们有一个模型来建模句子的长度n，但是更加简单的办法是引入一个特殊的句子结束标签\</s>来表示句子的结束。从生成模型的角度来说，我们先预测第一个词，然后用第一个词预测第二个词(假设是bigram)，然后用第二个词预测第三个词，……，直到遇到\</s>，我们结束这个过程。这样的模型能够保证所有可能句子的概率加起来是1，也就是一个合法的概率分布。

但是前面一个问题就是预测第一个词没有history，虽然我们记为$p(w_1)$，但是它不是词$w_1$出现的概率，而是词$w_1$出现在第一个位置的概率！为了统一记号，我们引入一个特殊的句子开始标签\<s>，这样记为$p(w_1 \vert \<s>)$，它表示词$w_1$出现在\<s>后的概率，也就是$w_1$出现在第一个位置的概率。

有了这两个特殊的"词"，我们的概率公式就更加简单：

$$
P(W) = P(w_1 | \<s>) \times P(w_2 |w_1) \times \ldots
\times P(w_n | w_{n-1}) \times P(\</s> | w_n)
$$

### N-gram的概率估计

N-gram的条件概率可以简单的统计其相对频率来得到。假设$c(w_1 \dots w_k)$是k-gram $w_1 \dots w_k$出现的频次。那么"bites"出现在"dog"之后的概率为：

$$
P(bites| dog) = { c(dog\ bites) \over c(dog) }
$$

请读者证明：

$$
P(bites | dog ) + P(bite | dog) + P(wags | dog) + \cdots
$$

的和为1，也就是dog为条件的所有概率加起来是1，从而保证这确实是一个合法的概率分布。

更加一般的，k-gram的概率可以使用如下公式来估计：

$$
P(w_k | w_1 \ldots w_{k-1}) = { c(w_1 \ldots w_k) \over c(w_1
\ldots w_{k-1})}
$$

### N-gram的平滑和打折

上面的相对频次估计有一个很严重的问题：在训练数据中没有出现过的N-gram的概率是0。训练数据是有限的，它不可能覆盖所有可能的N-gram。此外对于语音识别系统来说，说话人一边思考一边说话，很多都不完全合乎语法。


因此我们需要给那些没有在训练数据中出现过的N-gram非零的概率，这可以通过语言模型平滑来实现。这是语言模型研究的一个重要子课题，[wiki](http://mlwiki.org/index.php/Smoothing_for_Language_Models)列举了常见的平滑方法，这些方法大部分都在SRILM里又被实现。我们这里只讨论一种方法——Witten-Bell平滑。选择它的原因有二：首先它比较容易理解和实现；其次和其它更复杂的方法相比它不需要那么多对数据分布的附加假设，因此它也更加鲁棒。
 

Witten-Bell平滑的基本思想是把未出现过的词也加入计数。那么有多少"未出现"过的词呢？这似乎有的困难：它都没出现，我怎么知道呢？Witten-Bell平滑是这么处理的：在训练数据里，第一次出现某个词前，它是未出现过的。因此对于训练数据来说，未出现过的词的个数就是训练数据中出现的词的个数，因此对于1-gram，我们可以这样估计其概率：

$$
\hat{P}(w)={c(w) \over c(.)+V}
$$
 

这里$c(w)$是词w出现的词数，而$c(.)$是所有词出现的词数和(也就是训练数据的长度)，V是第一个词出现的词的个数，也就是词典的大小。和未平滑相比，分母多了一个V，因此所有出现过的词的概率加起来是小于1的。因此这种平滑叫做打折(discount)，把见过的n-gram的概率打个折扣，多出来的概率分给未见过的词。未见过的词的概率是：

$$
P(unseenword)={V \over c(.)+V}
$$

注意：未出现的词有很多个，它们的概率加起来是上面的概率，而不是每一个概率都是上面的值(否则加起来大于一了)。下一节我们会介绍未出现过的词怎么分配这些概率。

从1-gram推广到k-gram是类似的：

$$
\hat{P}(w_k|w_1…w_{k−1}) = {c(w_1…w_k) \over c(w_1…w_{k−1})+V(w_1…w_{k−1}⋅)}
$$

其中$c(w_1…w_{k-1})$是k-1 gram出现的次数，而$V(w_1…w_{k−1}⋅)$是$w_1…w_{k−1}$之后的不同词的个数。

比如训练数据为：

```
a b c a b d a b d
```

则$c(ab)=3$，而$V(ab⋅)=\vert \{c, d, d\} \vert=2$。也就是ab之后出现的词为[c,d,d]，不同的词只有[c,d]两个。
